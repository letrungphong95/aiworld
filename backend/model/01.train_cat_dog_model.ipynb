{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  01. Train a Cat Dog classification model based on the (pretrained) VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "keras = tf.keras\n",
    "\n",
    "# default params\n",
    "_IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Download dataset and Convert to .tflite\n",
    "\n",
    "Url: https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and unzip the dataset at `data/PetImages`\n",
    "if os.path.isdir('data/PetImages') == False:\n",
    "    print(\"Unzip the dataset\")\n",
    "    zip_ref = ZipFile('kagglecatsanddogs_3367a.zip', 'r')\n",
    "    zip_ref.extractall('data/')\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to first 20K images for train set and last 5K images for validation set.\n",
    "dataset_dir = 'data/PetImages/'\n",
    "\n",
    "# get all image urls\n",
    "image_paths = glob.glob(os.path.join(dataset_dir, '*/*.jpg'))\n",
    "\n",
    "# shuffle image urls\n",
    "random.seed(420)\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# split into train/val sets with ratio of 20K:5K\n",
    "dataset = {\n",
    "    'train': image_paths[:-5000],\n",
    "    'val': image_paths[-5000:]\n",
    "}\n",
    "\n",
    "print(\"Dataset has been seperated into train and val sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tfrecord files for the dataset\n",
    "# Helper functions\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def to_example(image_string, label):\n",
    "    \"\"\" Create a dictionary with features (tf.Example) from the given image and label.\"\"\"\n",
    "    image_shape = tf.image.decode_jpeg(image_string).shape\n",
    "    feature = {\n",
    "        'height': _int64_feature(int(0.95*image_shape[0])),\n",
    "        'width': _int64_feature(int(0.95*image_shape[1])),\n",
    "        'label': _int64_feature(label),\n",
    "        'image_raw': _bytes_feature(image_string),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def write_tfrecord(filenames, path, prefix='train'):\n",
    "    \"\"\" Load images from given `filenames` and write into a tfrecord file.\\\n",
    "    \n",
    "    Args:\n",
    "    - filenames: list -- include image filenames/urls.\n",
    "    - path: str -- where to save the tfrecord file.\n",
    "    - prefix: str -- prefix for the name of the tfrecord file, e.g.: `train` or `val`.\n",
    "    \n",
    "    Return:\n",
    "    - None.\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(path, prefix+'.tfrecord')\n",
    "    with tf.io.TFRecordWriter(save_path) as writer:\n",
    "        num_image = 0\n",
    "        for filename in filenames:\n",
    "            label = 0 if 'Cat' in filename else 1\n",
    "            image_string = open(filename, 'rb').read()\n",
    "            try:\n",
    "                tf_example = to_example(image_string, label)\n",
    "            except: # just ignore some corrupted images\n",
    "                continue\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            num_image += 1\n",
    "    print(\"Create tfrecord at: \\n  - {}\".format(os.path.join(path, prefix+'.tfrecord')))\n",
    "    \n",
    "# create tfrecord files for train and val sets, respectively.\n",
    "for key in dataset.keys():\n",
    "    write_tfrecord(dataset[key], 'data/PetImages/', key)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def _parse_function(example_proto):\n",
    "    \"\"\" Parse the data from given `example_proto`. \"\"\"\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    image_string = parsed_example['image_raw']\n",
    "    label = parsed_example['label']\n",
    "    height = parsed_example['height']\n",
    "    weight = parsed_example['width']\n",
    "    \n",
    "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image_decoded, tf.float32) # convert to float values in [0, 1]\n",
    "    \n",
    "    return image, label, height, weight\n",
    "\n",
    "def _augment_image(image, label, height, weight):\n",
    "    \"\"\" Augment image for training.\"\"\"    \n",
    "    image = tf.image.random_flip_left_right(image)        \n",
    "    image = tf.image.random_crop(image, [height, weight, 3])\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)                \n",
    "    \n",
    "    return image, label, height, weight\n",
    "      \n",
    "def _resize_image(image, label, height, weight, size):\n",
    "    \"\"\" Resize image to meet the input size of the classification model. \"\"\"\n",
    "    resized_image = tf.image.resize_with_pad(image, size, size)   \n",
    "\n",
    "    return resized_image, label\n",
    "\n",
    "def _preprocess_image(image, label):\n",
    "    \"\"\" Preprocess image to meet the VGG16 image preprocessing method.\"\"\"\n",
    "    # scale to [0, 255]\n",
    "    preprocessed_image = 255.0*image\n",
    "    \n",
    "    # convert RGB to BGR\n",
    "    preprocessed_image = preprocessed_image[...,::-1]\n",
    "    \n",
    "    # subtract the mean\n",
    "    preprocessed_image = preprocessed_image - [103.939, 116.779, 123.68]\n",
    "    \n",
    "    return preprocessed_image, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "# train data pipeline\n",
    "train_dataset = (tf.data.TFRecordDataset('data/PetImages/train.tfrecord')\n",
    "                     .map(_parse_function)\n",
    "                     .map(_augment_image)\n",
    "                     .map(lambda im, l, h, w: _resize_image(im, l, h, w, size=_IMAGE_SIZE))\n",
    "                     .map(_preprocess_image)\n",
    "                     .shuffle(1000)\n",
    "                     .batch(32)\n",
    "                     .prefetch(1)  # make sure you always have one batch ready to serve\n",
    "                )\n",
    "\n",
    "# val data pipeline\n",
    "val_dataset = (tf.data.TFRecordDataset('data/PetImages/val.tfrecord')\n",
    "                     .map(_parse_function)\n",
    "                     .map(lambda im, l, h, w: _resize_image(im, l, h, w, size=_IMAGE_SIZE))\n",
    "                     .map(_preprocess_image)\n",
    "                     .batch(32)\n",
    "                     .prefetch(1)  # make sure you always have one batch ready to serve                \n",
    "              )\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train VGG16 Cat Dog model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 512)         262656    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 256)         131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         32896     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 15,141,826\n",
      "Trainable params: 15,141,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_fn(num_class=None, image_size=None):\n",
    "    \"\"\"\n",
    "    This function creates initiative CNN model using VGG16 convolutional layer \n",
    "    and add some new layer on top to finetune our Dataset.  \n",
    "    \n",
    "    Arguments:\n",
    "    -num_class: int, number of class for output of our model \n",
    "    (E.g: num_class=2 for Cat and Dog dataset)\n",
    "    -image_size: int, size of input image from dataset for our model \n",
    "    \n",
    "    Returns:\n",
    "    - model: keras sequential model class, intiative model from keras   \n",
    "    \"\"\"\n",
    "    # Define keras model \n",
    "    model = keras.models.Sequential()\n",
    "    # Add the vgg16 convolutional base model\n",
    "    model.add(keras.applications.VGG16(weights='imagenet', \n",
    "                                       include_top=False, \n",
    "                                       input_shape=(image_size, image_size, 3)))\n",
    "    # Add new layers on top\n",
    "    model.add(keras.layers.Conv2D(512, 1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(256, 1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(128, 1))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(keras.layers.Dense(num_class, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create initiative model\n",
    "model = model_fn(num_class=2, image_size=_IMAGE_SIZE)\n",
    "\n",
    "# Define model with optimizer method and loss function\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the dog/cat prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 53s 535ms/step - loss: 0.1113 - accuracy: 0.9506\n",
      "Val loss = 0.0610, val acc = 0.9771\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=tf.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model \n",
    "model.fit_generator(train_dataset, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch = 100)\n",
    "\n",
    "# Evaluate on the val dataset \n",
    "val_loss, val_acc = model.evaluate_generator(val_dataset)\n",
    "print(\"Val loss = {:.4f}, val acc = {:.4f}\".format(val_loss, val_acc))\n",
    "\n",
    "# Save keras model to .h5 file after training \n",
    "model.save('experiments/VGG16_based_classification/vgg16_catdog.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path, size):\n",
    "    \"\"\"\n",
    "    This function preprocess test image for prediction\n",
    "    Arguments:\n",
    "    -path: \n",
    "    -size:\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read image from image path\n",
    "    image_decoded = mpimg.imread(path)\n",
    "    image = tf.image.convert_image_dtype(image_decoded, tf.float32) # convert to float values in [0, 1]\n",
    "    # resize image to fit the input size\n",
    "    resized_image = tf.image.resize_with_pad(image, size, size)\n",
    "    # Scale to [0,255]\n",
    "    resized_image = resized_image*255\n",
    "    # Transpote from RGB to BGR then subtract for mean(imagenet)\n",
    "    resized_image = resized_image[...,::-1] - [103.939,116.779,123.68]\n",
    "    # Reshape tensor\n",
    "    resized_image = tf.reshape(resized_image,[1,224,224,3])\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Cat','Dog']\n",
    "test_path = 'data/PetImages/Dog/12153.jpg'\n",
    "# Read image to array (height,weight,3)\n",
    "raw_image = mpimg.imread(test_path)\n",
    "# Preprocess image \n",
    "test_image = preprocess_image(path=test_path, size=224)\n",
    "# Predict \n",
    "result = model.predict(test_image)\n",
    "print('Predict: ',label[result.argmax()])\n",
    "print('Confidence: ',result.max())\n",
    "plt.imshow(raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
